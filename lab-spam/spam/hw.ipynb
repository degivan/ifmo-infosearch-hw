{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ** Описание **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)\n",
    "        \n",
    "def trace_worker(items_num, worker_id, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d in worker_id %d\" % (items_num, worker_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Декораторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text, unicode): text = text.encode('utf8')\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, unicode): text = text.decode('utf8')\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp\n",
    "\n",
    "#P.S. Декораторы могут усложнять отладку, так что от них вполне можно отказаться и воспользоваться copy-paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение текста из html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Извлечение текста при помощи встроенных модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import re\n",
    "\n",
    "###Извлечение текста из title можно вписать сюда\n",
    "\n",
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = \"\"\n",
    "        self._in_title = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        text = data.strip()\n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            self._text.append(text + ' ')\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True\n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):\n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self._text).strip()\n",
    "\n",
    "@convert2unicode\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return parser.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html2text = html2text_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы для токенизации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = unicode()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = unicode()\n",
    "    if word: yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "#hint, чтобы установка pymorphy2 не была бы обязательной\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "@convert2lower\n",
    "@convert2unicode\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основная функция, которая вызывается для преобразования html в список слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text, tokenizer=easy_tokenizer):\n",
    "    return tokenizer(to_text(raw_html).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "PreDocItem = namedtuple('PreDocItem', ['doc_id', 'is_spam', 'url', 'html_data'])\n",
    "\n",
    "def load_csv(input_file_name):    \n",
    "    \"\"\"\n",
    "    Загружаем данные и извлекаем на лету признаки\n",
    "    Сам контент не сохраняется, чтобы уменьшить потребление памяти - чтобы\n",
    "    можно было запускать даже на ноутбуках в классе\n",
    "    \"\"\"\n",
    "    predocs = []\n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = int(parts[1])                   \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            predocs.append(PreDocItem(url_id, mark, url, html_data))                  \n",
    "        trace(i, 1)\n",
    "    return predocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:28:33 INFO:Complete items 00000\n",
      "18:28:33 INFO:Complete items 01000\n",
      "18:28:33 INFO:Complete items 02000\n",
      "18:28:34 INFO:Complete items 03000\n",
      "18:28:34 INFO:Complete items 04000\n",
      "18:28:34 INFO:Complete items 05000\n",
      "18:28:35 INFO:Complete items 06000\n",
      "18:28:35 INFO:Complete items 07000\n",
      "18:28:35 INFO:Complete items 07043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.03 s, sys: 234 ms, total: 2.27 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_DATA_FILE  = '/home/vanyadeg/dev/ifmo-infosearch-hw/lab-spam/data/kaggle_train_data_tab.csv'\n",
    "\n",
    "train_predocs = list(load_csv(TRAIN_DATA_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:28:35 INFO:Complete items 00000\n",
      "18:28:35 INFO:Complete items 01000\n",
      "18:28:36 INFO:Complete items 02000\n",
      "18:28:36 INFO:Complete items 03000\n",
      "18:28:37 INFO:Complete items 04000\n",
      "18:28:37 INFO:Complete items 05000\n",
      "18:28:37 INFO:Complete items 06000\n",
      "18:28:38 INFO:Complete items 07000\n",
      "18:28:38 INFO:Complete items 08000\n",
      "18:28:38 INFO:Complete items 09000\n",
      "18:28:39 INFO:Complete items 10000\n",
      "18:28:39 INFO:Complete items 11000\n",
      "18:28:39 INFO:Complete items 12000\n",
      "18:28:40 INFO:Complete items 13000\n",
      "18:28:40 INFO:Complete items 14000\n",
      "18:28:40 INFO:Complete items 15000\n",
      "18:28:41 INFO:Complete items 16000\n",
      "18:28:41 INFO:Complete items 16038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.37 s, sys: 581 ms, total: 5.95 s\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TEST_DATA_FILE  = '/home/vanyadeg/dev/ifmo-infosearch-hw/lab-spam/data/kaggle_test_data_tab.csv'\n",
    "\n",
    "test_predocs = load_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def count_avg(words):\n",
    "    avg_word_len = 0\n",
    "    for word in words:\n",
    "        avg_word_len += len(word)\n",
    "    return avg_word_len / len(words)\n",
    "\n",
    "def count_in_tag(html_data, pattern):\n",
    "    res = 0\n",
    "    anchors = re.findall(pattern, html_data)\n",
    "    for anchor in anchors:\n",
    "        res += len(list(html2word(anchor)))\n",
    "    return res\n",
    "    \n",
    "def calc_features(predocs):\n",
    "    vectorizer = TfidfVectorizer(min_df=0.04)\n",
    "    X = vectorizer.fit_transform([pd.html_data for pd in predocs]).toarray()\n",
    "    for i, pd in enumerate(predocs):\n",
    "        features = X[i]\n",
    "        yield DocItem(pd.doc_id, pd.is_spam, pd.url, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = len(train_predocs)\n",
    "test_size = len(test_predocs)\n",
    "\n",
    "docs = list(calc_features(train_predocs + test_predocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7044\n",
      "16039\n",
      "2981\n"
     ]
    }
   ],
   "source": [
    "train_docs = docs[0 : train_size]\n",
    "test_docs = docs[train_size:]\n",
    "print len(train_docs)\n",
    "print len(test_docs)\n",
    "print len(docs[0].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "class Classifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clf = AdaBoostClassifier(n_estimators=500, learning_rate=0.8)\n",
    "\n",
    "    def predict(self, doc):        \n",
    "        return self.clf.predict([doc.features])[0]                     \n",
    "    \n",
    "    def predict_all(self, docs):\n",
    "        res = []\n",
    "        for doc_num, doc in enumerate(docs):\n",
    "            trace(doc_num)\n",
    "            prediction = self.predict(doc)            \n",
    "            res.append( (doc.doc_id, doc.is_spam, doc.url, prediction) )\n",
    "        return res\n",
    "    \n",
    "    def train(self, docs): \n",
    "        self.clf.fit([d.features for d in docs], [d.is_spam for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_docs[:int(len(train_docs)*0.8)]\n",
    "test = train_docs[int(len(train_docs)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([d.features for d in train])\n",
    "y_train = np.array([d.is_spam for d in train])\n",
    "X_test = np.array([d.features for d in test])\n",
    "y_test = np.array([d.is_spam for d in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full mode\n",
    "\n",
    "X_train = np.array([d.features for d in train_docs])\n",
    "y_train = np.array([d.is_spam for d in train_docs])\n",
    "X_test = np.array([d.features for d in test_docs])\n",
    "y_test = np.array([d.is_spam for d in test_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 5,  \n",
    "    'eta': 0.1,  \n",
    "    'silent': 1, \n",
    "    'objective': 'multi:softprob', \n",
    "    'num_class': 2} \n",
    "num_round = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16039\n"
     ]
    }
   ],
   "source": [
    "with open('my_submission_xgb7_004_5_500.csv' , 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    predictions = best_preds\n",
    "    print len(predictions)\n",
    "    for doc_id, res in zip([doc.doc_id for doc in test_docs], predictions):\n",
    "        writer.writerow([doc_id, int(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
